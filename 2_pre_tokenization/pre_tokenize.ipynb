{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20117de5",
   "metadata": {},
   "source": [
    "### We keep the same example 'sentence' :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaac8437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello my name is Nathan, i'm a french computer science student.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_name = \"Nathan\" # replace with your own name :)\n",
    "sentence = f\"Hello my name is {your_name}, i'm a french computer science student.\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad8786",
   "metadata": {},
   "source": [
    "# Pre-Tokenizer:\n",
    "- A tokenizer cannot be trained on raw text alone. \n",
    "- Instead, we first need to split the texts into small entities, like words. \n",
    "\n",
    "- A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95a1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# to simplify some the call of our model, we will use these functions.\n",
    "\n",
    "def tokenizer_function(model_checkpoint):\n",
    "    return AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "#\n",
    "def tokenizer_pre_tokenize_str(tokenizer, sentence):\n",
    "    return tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74574f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('my', (6, 8)),\n",
       " ('name', (9, 13)),\n",
       " ('is', (14, 16)),\n",
       " ('Nathan', (17, 23)),\n",
       " (',', (23, 24)),\n",
       " ('i', (25, 26)),\n",
       " (\"'\", (26, 27)),\n",
       " ('m', (27, 28)),\n",
       " ('a', (29, 30)),\n",
       " ('french', (31, 37)),\n",
       " ('computer', (38, 46)),\n",
       " ('science', (47, 54)),\n",
       " ('student', (55, 62)),\n",
       " ('.', (62, 63))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'bert-base-uncased'\n",
    "tokenizer = tokenizer_function(model_checkpoint)\n",
    "\n",
    "pre_tokenize_sentence = tokenizer_pre_tokenize_str(tokenizer, sentence)\n",
    "pre_tokenize_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd7399",
   "metadata": {},
   "source": [
    "You can see each word of our sentence is divided now and we have also access to offset foreach word. \n",
    "\n",
    "Offset -> index of the word in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9576773",
   "metadata": {},
   "source": [
    "### Now let's try another model with the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38b21b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Ġmy, Ġname, Ġis, ĠNathan, ,, Ġi, 'm, Ġa, Ġfrench, Ġcomputer, Ġscience, Ġstudent, .\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 'gpt2'\n",
    "tokenizer = tokenizer_function(model_checkpoint)\n",
    "\n",
    "pre_tokenize_sentence = tokenizer_pre_tokenize_str(tokenizer, sentence)\n",
    "s = \"\"\n",
    "for i, (word, offset) in enumerate(pre_tokenize_sentence):\n",
    "    s += word\n",
    "    if i < len(pre_tokenize_sentence) - 1:\n",
    "        s += ', '\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1239720",
   "metadata": {},
   "source": [
    "We can see that with gpt2, each 'new' word start with 'Ġ'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7eb5a",
   "metadata": {},
   "source": [
    "### Let's try another model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0a2266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello', (0, 5)),\n",
       " ('▁my', (6, 8)),\n",
       " ('▁name', (9, 13)),\n",
       " ('▁is', (14, 16)),\n",
       " ('▁Nathan,', (17, 24)),\n",
       " (\"▁i'm\", (25, 28)),\n",
       " ('▁a', (29, 30)),\n",
       " ('▁french', (31, 37)),\n",
       " ('▁computer', (38, 46)),\n",
       " ('▁science', (47, 54)),\n",
       " ('▁student.', (55, 63))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 't5-small'\n",
    "tokenizer = tokenizer_function(model_checkpoint)\n",
    "\n",
    "pre_tokenize_sentence = tokenizer_pre_tokenize_str(tokenizer, sentence)\n",
    "pre_tokenize_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334042e4",
   "metadata": {},
   "source": [
    "This time, each word even the first one, start with '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9000a0",
   "metadata": {},
   "source": [
    "### In conclusion, each model is train to pre-tokenize differently.\n",
    "#### Now that we see Normalization and Pre-Tokenization, we can reach the next step of the tokenizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
